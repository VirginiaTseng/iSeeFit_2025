#!/usr/bin/env python3
"""
ç®€åŒ–çš„ PyTorch åˆ° Core ML è½¬æ¢è„šæœ¬
ä¸“é—¨å¤„ç†æ¨¡å‹ç»“æ„ä¸åŒ¹é…çš„é—®é¢˜
"""

import torch
import coremltools as ct
import torchvision.models as M
import torch.nn as nn
import os
from pathlib import Path

def load_model_safely(model_path, num_classes=101):
    """å®‰å…¨åŠ è½½æ¨¡å‹ï¼Œå¤„ç†å„ç§å¯èƒ½çš„ä¿å­˜æ ¼å¼"""
    print(f"Loading model from: {model_path}")
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    
    # åˆ›å»ºåŸºç¡€æ¨¡å‹
    model = M.vit_b_16(weights=None)
    model.heads.head = nn.Linear(model.heads.head.in_features, num_classes)
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(model_path, map_location="cpu")
    print(f"Checkpoint keys: {list(checkpoint.keys())}")
    
    # å°è¯•ä¸åŒçš„åŠ è½½æ–¹å¼
    if isinstance(checkpoint, dict):
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        elif 'model' in checkpoint:
            state_dict = checkpoint['model']
        else:
            state_dict = checkpoint
    else:
        state_dict = checkpoint
    
    # åŠ è½½çŠ¶æ€å­—å…¸ï¼Œå…è®¸éƒ¨åˆ†åŒ¹é…
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    
    if missing_keys:
        print(f"Missing keys: {missing_keys[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
    if unexpected_keys:
        print(f"Unexpected keys: {unexpected_keys[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
    
    model.eval()
    return model

def convert_to_coreml(model, output_path="Food101_ViT.mlmodel"):
    """è½¬æ¢ä¸º Core ML æ ¼å¼"""
    print("Converting to Core ML...")
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    example_input = torch.randn(1, 3, 224, 224)
    
    # å°è¯•ä¸åŒçš„è½¬æ¢æ–¹æ³•
    try:
        # æ–¹æ³•1: JIT trace
        print("Trying JIT trace...")
        traced_model = torch.jit.trace(model, example_input)
        
        # è½¬æ¢ä¸º Core ML
        mlmodel = ct.convert(
            traced_model,
            inputs=[ct.ImageType(
                name="input_image",
                shape=example_input.shape,
                scale=1/255.0,
                bias=[-0.485/0.229, -0.456/0.224, -0.406/0.225]
            )],
            convert_to="mlprogram"
        )
        
    except Exception as e1:
        print(f"JIT trace failed: {e1}")
        try:
            # æ–¹æ³•2: JIT script
            print("Trying JIT script...")
            scripted_model = torch.jit.script(model)
            
            mlmodel = ct.convert(
                scripted_model,
                inputs=[ct.ImageType(
                    name="input_image",
                    shape=example_input.shape,
                    scale=1/255.0,
                    bias=[-0.485/0.229, -0.456/0.224, -0.406/0.225]
                )],
                convert_to="neuralnetwork"  # ä½¿ç”¨æ›´å…¼å®¹çš„æ ¼å¼
            )
            
        except Exception as e2:
            print(f"JIT script failed: {e2}")
            try:
                # æ–¹æ³•3: ç›´æ¥è½¬æ¢ï¼ˆä¸æ¨èï¼Œä½†æœ‰æ—¶æœ‰æ•ˆï¼‰
                print("Trying direct conversion...")
                mlmodel = ct.convert(
                    model,
                    inputs=[ct.ImageType(
                        name="input_image",
                        shape=example_input.shape
                    )],
                    convert_to="neuralnetwork"
                )
            except Exception as e3:
                raise Exception(f"All conversion methods failed: {e1}, {e2}, {e3}")
    
    # ä¿å­˜æ¨¡å‹
    print(f"Saving to {output_path}...")
    mlmodel.save(output_path)
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    file_size = os.path.getsize(output_path) / (1024 * 1024)
    print(f"âœ… Model saved successfully!")
    print(f"ğŸ“ File: {output_path}")
    print(f"ğŸ“ Size: {file_size:.2f} MB")
    
    return mlmodel

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Starting PyTorch to Core ML conversion...")
    
    # è®¾ç½®è·¯å¾„
    model_path = "checkpoints/best_model.pt"
    output_path = "Food101_ViT.mlmodel"
    
    try:
        # åŠ è½½æ¨¡å‹
        model = load_model_safely(model_path)
        print("âœ… Model loaded successfully!")
        
        # æµ‹è¯•æ¨¡å‹
        print("Testing model...")
        test_input = torch.randn(1, 3, 224, 224)
        with torch.no_grad():
            output = model(test_input)
        print(f"Model output shape: {output.shape}")
        
        # è½¬æ¢ä¸º Core ML
        mlmodel = convert_to_coreml(model, output_path)
        
        print("ğŸ‰ Conversion completed successfully!")
        
    except Exception as e:
        print(f"âŒ Conversion failed: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
